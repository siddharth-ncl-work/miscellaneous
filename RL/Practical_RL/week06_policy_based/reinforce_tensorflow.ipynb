{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in TensorFlow (3 pts)\n",
    "\n",
    "Just like we did before for Q-learning, this time we'll design a TensorFlow network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    %tensorflow_version 1.x\n",
    "    \n",
    "    if not os.path.exists('.setup_complete'):\n",
    "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "        !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5b796d1710>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env, '_max_episode_steps'):\n",
    "    env = env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input variables. We only need <s, a, r> for REINFORCE\n",
    "ph_states = tf.placeholder('float32', (None,) + state_dim, name=\"states\")\n",
    "ph_actions = tf.placeholder('int32', name=\"action_ids\")\n",
    "ph_cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#<YOUR CODE: define network graph using raw TF, Keras, or any other library you prefer>\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(128,activation='relu',input_shape=(state_dim)))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(n_actions,activation='linear'))\n",
    "\n",
    "logits = model(ph_states)#<YOUR CODE: symbolic outputs of your network _before_ softmax>\n",
    "\n",
    "policy = tf.nn.softmax(logits)\n",
    "log_policy = tf.nn.log_softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model parameters\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    return policy.eval({ph_states: [states]})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000):\n",
    "    \"\"\" \n",
    "    Play a full session with REINFORCE agent.\n",
    "    Returns sequences of states, actions, and rewards.\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(s)\n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "        a = np.random.choice(np.arange(n_actions),p=action_probs)#<YOUR CODE>\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
    "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
    "&= r_t + \\gamma * G_{t + 1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Take a list of immediate rewards r(s,a) for the whole session \n",
    "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
    "    \n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    A simple way to compute cumulative rewards is to iterate from the last\n",
    "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    #<YOUR CODE>\n",
    "    returns=np.zeros(len(rewards))\n",
    "    _return=0\n",
    "    i=len(rewards)-1\n",
    "    for reward in rewards[-1::-1]:\n",
    "        _return=reward+gamma*_return\n",
    "        returns[i]=_return\n",
    "        i-=1\n",
    "    return returns#<YOUR CODE: array of cumulative rewards>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=list(range(10))\n",
    "l[1:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "assert len(get_cumulative_rewards(range(100))) == 100\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
    "    [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
    "\n",
    "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
    "\n",
    "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "We can abuse Tensorflow's capabilities for automatic differentiation by defining our objective function as follows:\n",
    "\n",
    "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code selects the log-probabilities (log pi(a_i|s_i)) for those actions that were actually played.\n",
    "indices = tf.stack([tf.range(tf.shape(log_policy)[0]), ph_actions], axis=-1)\n",
    "log_policy_for_actions = tf.gather_nd(log_policy, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy objective as in the last formula. Please use reduce_mean, not reduce_sum.\n",
    "# You may use log_policy_for_actions to get log probabilities for actions taken.\n",
    "# Also recall that we defined ph_cumulative_rewards earlier.\n",
    "\n",
    "J = tf.reduce_mean(log_policy_for_actions*ph_cumulative_rewards)#<YOUR CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, for a discrete probability distribution (like the one our policy outputs), entropy is defined as:\n",
    "\n",
    "$$ \\operatorname{entropy}(p) = -\\sum_{i = 1}^n p_i \\cdot \\log p_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy regularization. If you don't add it, the policy will quickly deteriorate to\n",
    "# being deterministic, harming exploration.\n",
    "_policy_for_actions = tf.gather_nd(policy, indices)\n",
    "_log_policy_for_actions=tf.log(_policy_for_actions)\n",
    "entropy = 0#-1*tf.reduce_sum(_policy_for_actions*_log_policy_for_actions)\n",
    "#entropy = -1*tf.reduce_sum(policy*log_policy)#<YOUR CODE: compute entropy. Do not forget the sign!>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Maximizing X is the same as minimizing -X, hence the sign.\n",
    "loss = -(J + 0.1 * entropy)\n",
    "\n",
    "update = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_session(states, actions, rewards, t_max=1000):\n",
    "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
    "    cumulative_rewards = get_cumulative_rewards(rewards)\n",
    "    update.run({\n",
    "        ph_states: states,\n",
    "        ph_actions: actions,\n",
    "        ph_cumulative_rewards: cumulative_rewards,\n",
    "    })\n",
    "    return sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer parameters\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 28.880\n",
      "mean reward: 74.390\n",
      "mean reward: 278.740\n",
      "mean reward: 694.730\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
    "\n",
    "    print(\"mean reward: %.3f\" % (np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record sessions\n",
    "\n",
    "import gym.wrappers\n",
    "\n",
    "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
    "    sessions = [generate_session(env_monitor) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos/openaigym.video.1.1337.video000027.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_names[-2]))  # You can also try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
